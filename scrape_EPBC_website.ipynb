{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the Python modules necessary for this notebook to run. These can be installed using, for instance, `pip` or `conda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:05.143280Z",
     "start_time": "2021-03-12T07:06:04.394180Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Handles maths\n",
    "import pandas as pd # Good for tables of data\n",
    "import matplotlib.pyplot as plt # Handles graphing\n",
    "import xarray as xr # Helpful for spatial data\n",
    "import requests # Downloads webpages\n",
    "from bs4 import BeautifulSoup # For parsing webpages\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os, sys\n",
    "import time\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:05.146963Z",
     "start_time": "2021-03-12T07:06:05.145027Z"
    }
   },
   "outputs": [],
   "source": [
    "# stored_table = pd.read_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv')\n",
    "# stored_table[stored_table['Reference Number'] == '2019/8527']\n",
    "# stored_table = stored_table.drop(1539, axis=0).reset_index(drop=True)\n",
    "# stored_table.to_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:05.150939Z",
     "start_time": "2021-03-12T07:06:05.148810Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://epbcnotices.environment.gov.au/publicnoticesreferrals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:11.682439Z",
     "start_time": "2021-03-12T07:06:05.153180Z"
    }
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless') # Comment out to see the actions on website\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "base_dir = '/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/files'\n",
    "\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "  \"download.default_directory\": base_dir,\n",
    "  \"download.prompt_for_download\": False,\n",
    "  \"download.directory_upgrade\": True,\n",
    "  \"safebrowsing.enabled\": True,\n",
    "  \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome('/usr/bin/chromedriver', options=options)\n",
    "driver.get(url);\n",
    "time.sleep(4)\n",
    "\n",
    "def clean_columns(table):\n",
    "    name_dict = {}\n",
    "    clean_str = '  . Activate to sort in descending order'\n",
    "    for col in range(len(table.columns)): \n",
    "        name_dict[table.columns[col]] = table.columns[col].replace(clean_str, '')\n",
    "    return table.rename(name_dict, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:11.718635Z",
     "start_time": "2021-03-12T07:06:11.684162Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_page(driver, page_number, table, stored_table, exist):\n",
    "    \n",
    "    # Two modes - download or update mode.\n",
    "    # If in download mode, search forward, skipping files that already exist \n",
    "    # in table.\n",
    "    # If in update mode, search forward, but assume new entries to website\n",
    "    # appear first, so stop when a certain number of matches have occured. \n",
    "\n",
    "    xpath = '//a[@class=\"btn btn-default btn-xs\" '\n",
    "    xpath += 'and @href=\"#\" and @data-toggle=\"dropdown\"]'\n",
    "    details_buttons = driver.find_elements_by_xpath(xpath)\n",
    "\n",
    "    xpath = '//a[@class=\"details-link launch-modal\" '\n",
    "    xpath += 'and @href=\"#\" and @title=\"View Details\"]'\n",
    "    details_links = driver.find_elements_by_xpath(xpath)\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_elements_by_xpath(\n",
    "            '//a[@href=\"#\" and @data-page=\"' + str(page_number+1) + '\"]'\n",
    "        )[1]\n",
    "    except:\n",
    "        print('Last page reached.')\n",
    "        next_button = driver.find_elements_by_xpath(\n",
    "            '//a[@href=\"#\" and @data-page=\"' + str(page_number) + '\"]'\n",
    "        )[0]\n",
    "\n",
    "    # These will record the number of files and filenames for each submission\n",
    "    num_files = []\n",
    "    file_names = []\n",
    "\n",
    "    # Iterate over the 30 entries in the table on current page checking for files\n",
    "    for i in range(30):\n",
    "\n",
    "        # If already downloaded, skip this row\n",
    "        if exist[i]:\n",
    "            continue\n",
    "\n",
    "        if i < 29: \n",
    "            # Move to element i+1, as i may be blocked by Chrome download bar! \n",
    "            ActionChains(driver).move_to_element(details_buttons[i+1]).perform()\n",
    "            details_buttons[i].click()\n",
    "            time.sleep(.5)\n",
    "        else:\n",
    "            # Move to navigation bar, as i may be blocked by Chrome download bar!\n",
    "            ActionChains(driver).move_to_element(next_button).perform()\n",
    "            details_buttons[i].click()\n",
    "            time.sleep(.5)\n",
    "\n",
    "        ActionChains(driver).move_to_element(details_links[i]).perform()\n",
    "        details_links[i].click()\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        iframe = driver.find_elements_by_xpath(\n",
    "            '//section[@class=\"modal fade modal-form modal-form-details in\"]'\n",
    "            + '/div/div/div/iframe'\n",
    "        )\n",
    "        driver.switch_to.frame(iframe[0])\n",
    "\n",
    "        file_links = driver.find_elements_by_xpath(\n",
    "            \"//a[contains(@href, '/_entity/annotation/')]\"\n",
    "        )\n",
    "\n",
    "        subprocess.run('rm ' + base_dir +'/*.pdf', shell=True)\n",
    "        ref_num = table['Reference Number'].iloc[i].replace('/','')\n",
    "        date = table['Date of notice'].iloc[i].strftime('%d%m%Y')\n",
    "        \n",
    "        # If no file links, don't try to download    \n",
    "        if not file_links:\n",
    "            num_files.append(0)\n",
    "            file_names.append('')\n",
    "        # If file links, try to download\n",
    "        else:\n",
    "            num_files.append(len(file_links))\n",
    "\n",
    "            # Check if folder name already exists, if so append count\n",
    "            folder_name = ref_num + '_' + date\n",
    "            shell_cmd = 'find ' + base_dir + '/*' + folder_name + '* -maxdepth 1 '\n",
    "            shell_cmd += '-type d | wc -l > ' + base_dir + '/folder_count.txt'\n",
    "            subprocess.run(shell_cmd, shell=True)\n",
    "            folder_count = int(np.loadtxt(base_dir + '/folder_count.txt'))\n",
    "            subprocess.run('rm ' + base_dir + '/folder_count.txt', shell=True)\n",
    "            \n",
    "            # Remove duplicate links - otherwise download code below breaks\n",
    "            # when the same file downloads and overwrites itself, resulting\n",
    "            # in file_count == len(file_links) never being satisfied\n",
    "            unique_file_links = []\n",
    "            unique_file_links_html = []\n",
    "            file_links_html = set([x.get_attribute('innerHTML') for x in file_links])\n",
    "            for k in range(len(file_links)):\n",
    "                link_text = file_links[k].get_attribute('innerHTML')\n",
    "                if re.search('.pdf', link_text, re.IGNORECASE):\n",
    "                    if file_links[k].get_attribute('innerHTML') not in unique_file_links_html:\n",
    "                        unique_file_links.append(file_links[k])\n",
    "                        unique_file_links_html.append(\n",
    "                            file_links[k].get_attribute('innerHTML')\n",
    "                        )\n",
    "                else:\n",
    "                    print('Non pdf files found on page ' + str(page_number) + ', row ' + str(i+1) \n",
    "                        + '. Check manually.')\n",
    "            file_links = unique_file_links\n",
    "                                \n",
    "            if folder_count > 0:\n",
    "                if folder_count == 1:\n",
    "                    # Append '_1' to existing folder\n",
    "                    shell_cmd = 'mv ' + base_dir + \"/\" + folder_name + ' '\n",
    "                    shell_cmd += base_dir + \"/\" + folder_name + '_1'\n",
    "                    subprocess.run(shell_cmd, shell=True)\n",
    "                # Appead folder_count + 1 to new folder\n",
    "                folder_name += '_' + str(folder_count+1)\n",
    "            folder_path = base_dir + '/' + folder_name\n",
    "\n",
    "            successful = False\n",
    "            attempts = 0\n",
    "            while not successful:\n",
    "                if attempts > 0:\n",
    "                    print('Download on page ' + str(page_number) + ', row ' + str(i+1) \n",
    "                        + ' timed out too many times.')\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    for j in range(len(file_links)):\n",
    "                        file_links[j].click()\n",
    "                        time.sleep(0.25)\n",
    "\n",
    "                    # Wait for files to download\n",
    "                    file_count = 0\n",
    "                    iterations = 0\n",
    "                    while file_count < len(file_links):\n",
    "                        if iterations > 1200:\n",
    "                            attempts += 1\n",
    "                            raise RuntimeError('Download timed out.')\n",
    "                        time.sleep(1)\n",
    "                        shell_cmd = '''find ''' + base_dir + '/*.PDF '\n",
    "                        shell_cmd += '''-maxdepth 1 -exec sh -c 'mv \"$1\" \"${1%.PDF}.pdf\"' _ {} \\;'''\n",
    "                        subprocess.run(shell_cmd, shell=True)\n",
    "                        shell_cmd = 'find ' + base_dir + '/*.pdf '\n",
    "                        shell_cmd += '-type f -print | wc -l > ' \n",
    "                        shell_cmd += base_dir + '/num_files.txt'\n",
    "                        subprocess.run(shell_cmd, shell=True)\n",
    "                        file_count = int(np.loadtxt(base_dir + '/num_files.txt'))\n",
    "                        iterations += 1\n",
    "                        time.sleep(.5)\n",
    "                    subprocess.run('rm ' + base_dir + '/num_files.txt', shell=True)\n",
    "                    successful = True\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    attempts += 1\n",
    "                    time.sleep(1)\n",
    "\n",
    "            # After files downloaded, move them to appropriate folder\n",
    "            subprocess.run(['mkdir', folder_path])\n",
    "            shell_cmd = 'mv ' + base_dir + '/*.pdf ' + folder_path \n",
    "            subprocess.run(shell_cmd, shell=True)\n",
    "\n",
    "            # Record the filenames\n",
    "            shell_cmd = 'find ' + folder_path + '/*.pdf -maxdepth 1 -type f '\n",
    "            shell_cmd += '-printf \"%f\\n\" > ' + folder_path + '/file_names.txt'\n",
    "            subprocess.run(shell_cmd, shell=True)\n",
    "            with open(folder_path + '/file_names.txt') as f:\n",
    "                lines = f.readlines()\n",
    "            file_names.append(', '.join(lines).replace('\\n',''))\n",
    "            subprocess.run('rm ' + folder_path + '/file_names.txt', shell=True)\n",
    "\n",
    "            shell_cmd = 'pdfunite ' + folder_path + '/*.pdf ' + folder_path \n",
    "            shell_cmd += '/' + folder_name + '_combined.pdf'\n",
    "            subprocess.run(shell_cmd, shell=True)\n",
    "    \n",
    "        driver.switch_to.default_content()\n",
    "        xpath = '//section[@class=\"modal fade modal-form '\n",
    "        xpath += 'modal-form-details in\"]/div/div/div/button'\n",
    "        close_button = driver.find_elements_by_xpath(xpath)\n",
    "        close_button[0].click()\n",
    "        time.sleep(.5)\n",
    "        \n",
    "        # Append the downloaded row to the stored table and save\n",
    "        row = table.iloc[i]\n",
    "        stored_table = stored_table.append(row, ignore_index=True)\n",
    "        stored_table = stored_table.sort_values(\n",
    "            by='Date of notice', axis = 0, \n",
    "            ascending=False\n",
    "        )\n",
    "        stored_table = stored_table.reset_index(drop=True)\n",
    "        stored_table['Date of notice'] = stored_table['Date of notice'].apply(lambda x: x.strftime('%d/%m/%Y'))\n",
    "        stored_table.to_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv', index=False, header=True)\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:06:11.725949Z",
     "start_time": "2021-03-12T07:06:11.722215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-12T07:13:52.789101Z",
     "start_time": "2021-03-12T07:06:11.727284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,168):\n",
    "    \n",
    "    loading = True\n",
    "    attempts = 0\n",
    "    while loading:\n",
    "        if attempts > 30:\n",
    "            raise RuntimeError('Could not load website')\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source)\n",
    "            table = pd.read_html(soup.prettify())[0]            \n",
    "            if len(table) == 30:\n",
    "                loading = False\n",
    "        except:\n",
    "            attempts += 1\n",
    "\n",
    "    table = clean_columns(table)\n",
    "    table['Date of notice'] = pd.to_datetime(table['Date of notice'], dayfirst=True)\n",
    "    table.drop(labels='Actions', axis=1, inplace=True)  \n",
    "    \n",
    "    try:\n",
    "        stored_table = pd.read_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv')\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)\n",
    "        shared = pd.merge(table, stored_table, how='left', indicator='Exist')\n",
    "        shared['Exist'] = np.where(shared.Exist == 'both', True, False)\n",
    "        exist = shared['Exist']\n",
    "        del shared\n",
    "    except:\n",
    "        stored_table = table.iloc[0:0]\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)\n",
    "        exist = [False]*30\n",
    "        exist = pd.Series(exist,name='Exist')\n",
    "     \n",
    "    if np.any(~exist):\n",
    "        scrape_page(driver, i, table, stored_table, exist)\n",
    "    \n",
    "    try:\n",
    "        next_button = driver.find_elements_by_xpath(\n",
    "            '//a[@href=\"#\" and @data-page=\"' + str(i+1) + '\"]'\n",
    "        )[1]\n",
    "        ActionChains(driver).move_to_element(next_button).perform()\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print('Quitting.')\n",
    "    \n",
    "    del table, stored_table\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
