{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the Python modules necessary for this notebook to run. These can be installed using, for instance, `pip` or `conda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T01:09:34.275381Z",
     "start_time": "2021-03-06T01:09:33.156494Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Handles maths\n",
    "import pandas as pd # Good for tables of data\n",
    "import matplotlib.pyplot as plt # Handles graphing\n",
    "import xarray as xr # Helpful for spatial data\n",
    "import requests # Downloads webpages\n",
    "from bs4 import BeautifulSoup # For parsing webpages\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os, sys\n",
    "import time\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T01:09:34.832517Z",
     "start_time": "2021-03-06T01:09:34.829319Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://epbcnotices.environment.gov.au/publicnoticesreferrals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T01:09:42.576093Z",
     "start_time": "2021-03-06T01:09:35.959712Z"
    }
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless') # Comment out to see the actions on website\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "base_dir = '/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/files'\n",
    "\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "  \"download.default_directory\": base_dir,\n",
    "  \"download.prompt_for_download\": False,\n",
    "  \"download.directory_upgrade\": True,\n",
    "  \"safebrowsing.enabled\": True,\n",
    "  \"plugins.always_open_pdf_externally\": True\n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome('/usr/bin/chromedriver', options=options)\n",
    "driver.get(url);\n",
    "time.sleep(4)\n",
    "\n",
    "def clean_columns(table):\n",
    "    name_dict = {}\n",
    "    clean_str = '  . Activate to sort in descending order'\n",
    "    for col in range(len(table.columns)): \n",
    "        name_dict[table.columns[col]] = table.columns[col].replace(clean_str, '')\n",
    "    return table.rename(name_dict, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-06T01:12:00.125716Z",
     "start_time": "2021-03-06T01:12:00.030802Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_page(driver, page_number, table, stored_table, exist):\n",
    "    \n",
    "    # Two modes - download or update mode.\n",
    "    # If in download mode, search forward, skipping files that already exist \n",
    "    # in table.\n",
    "    # If in update mode, search forward, but assume new entries to website\n",
    "    # appear first, so stop when a certain number of matches have occured. \n",
    "\n",
    "    xpath = '//a[@class=\"btn btn-default btn-xs\" '\n",
    "    xpath += 'and @href=\"#\" and @data-toggle=\"dropdown\"]'\n",
    "    details_buttons = driver.find_elements_by_xpath(xpath)\n",
    "\n",
    "    xpath = '//a[@class=\"details-link launch-modal\" '\n",
    "    xpath += 'and @href=\"#\" and @title=\"View Details\"]'\n",
    "    details_links = driver.find_elements_by_xpath(xpath)\n",
    "\n",
    "    next_button = driver.find_elements_by_xpath(\n",
    "        '//a[@href=\"#\" and @data-page=\"' + str(page_number+1) + '\"]'\n",
    "    )[1]\n",
    "\n",
    "    # These will record the number of files and filenames for each submission\n",
    "    num_files = []\n",
    "    file_names = []\n",
    "\n",
    "    # Iterate over the 30 entries in the table on current page checking for files\n",
    "    for i in range(30):\n",
    "\n",
    "        # If already downloaded, skip this row\n",
    "        if exist[i]:\n",
    "            continue\n",
    "\n",
    "        if i < 29: \n",
    "            # Move to element i+1, as i may be blocked by Chrome download bar! \n",
    "            ActionChains(driver).move_to_element(details_buttons[i+1]).perform()\n",
    "            details_buttons[i].click()\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            # Move to navigation bar, as i may be blocked by Chrome download bar!\n",
    "            ActionChains(driver).move_to_element(next_button).perform()\n",
    "            details_buttons[i].click()\n",
    "            time.sleep(1)\n",
    "\n",
    "        ActionChains(driver).move_to_element(details_links[i]).perform()\n",
    "        details_links[i].click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        iframe = driver.find_elements_by_xpath(\n",
    "            '//section[@class=\"modal fade modal-form modal-form-details in\"]'\n",
    "            + '/div/div/div/iframe'\n",
    "        )\n",
    "        driver.switch_to.frame(iframe[0])\n",
    "\n",
    "        file_links = driver.find_elements_by_xpath(\n",
    "            \"//a[contains(@href, '/_entity/annotation/')]\"\n",
    "        )\n",
    "\n",
    "        subprocess.run('rm ' + base_dir +'/*.pdf', shell=True)\n",
    "        ref_num = table['Reference Number'].iloc[i].replace('/','')\n",
    "        date = table['Date of notice'].iloc[i].strftime('%d%m%Y')\n",
    "        \n",
    "        # If no files, skip this row    \n",
    "        if not file_links:\n",
    "            num_files.append(0)\n",
    "            file_names.append('')\n",
    "\n",
    "            driver.switch_to.default_content()\n",
    "            xpath = '//section[@class=\"modal fade modal-form '\n",
    "            xpath += 'modal-form-details in\"]/div/div/div/button'\n",
    "            close_button = driver.find_elements_by_xpath(xpath)\n",
    "            close_button[0].click()\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        num_files.append(len(file_links))\n",
    "\n",
    "        # Check if folder name already exists, if so append count\n",
    "        folder_name = ref_num + '_' + date\n",
    "        shell_cmd = 'find ' + base_dir + '/*' + folder_name + '* -maxdepth 1 '\n",
    "        shell_cmd += '-type d | wc -l > ' + base_dir + '/folder_count.txt'\n",
    "        subprocess.run(shell_cmd, shell=True)\n",
    "        folder_count = int(np.loadtxt(base_dir + '/folder_count.txt'))\n",
    "        subprocess.run('rm ' + base_dir + '/folder_count.txt', shell=True)\n",
    "\n",
    "        if folder_count > 0:\n",
    "            if folder_count == 1:\n",
    "                # Append '_1' to existing folder\n",
    "                shell_cmd = 'mv ' + base_dir + \"/\" + folder_name + ' '\n",
    "                shell_cmd += base_dir + \"/\" + folder_name + '_1'\n",
    "                subprocess.run(shell_cmd, shell=True)\n",
    "            # Appead folder_count + 1 to new folder\n",
    "            folder_name += '_' + str(folder_count+1)\n",
    "        folder_path = base_dir + '/' + folder_name\n",
    "        \n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        successful = False\n",
    "        attempts = 0\n",
    "        while not successful:\n",
    "            if attempts > 5:\n",
    "                raise RuntimeError('Download timed out too many times.')\n",
    "            try:\n",
    "                for j in range(len(file_links)):\n",
    "                    file_links[j].click()\n",
    "                    time.sleep(1)\n",
    "\n",
    "                # Wait for files to download\n",
    "                file_count = 0\n",
    "                iterations = 0\n",
    "                while file_count < len(file_links):\n",
    "                    if iterations > 600:\n",
    "                        raise RuntimeError('Download timed out.')\n",
    "                    time.sleep(1)\n",
    "                    shell_cmd = '''find ''' + base_dir + '''/*.PDF -maxdepth 1 -exec sh -c 'mv \"$1\" \"${1%.PDF}.pdf\"' _ {} \\;'''\n",
    "                    subprocess.run(shell_cmd, shell=True)\n",
    "                    shell_cmd = 'find ' + base_dir + '/*.pdf '\n",
    "                    shell_cmd += '-type f -print | wc -l > ' \n",
    "                    shell_cmd += base_dir + '/num_files.txt'\n",
    "                    subprocess.run(shell_cmd, shell=True)\n",
    "                    file_count = int(np.loadtxt(base_dir + '/num_files.txt'))\n",
    "                    iterations += 1\n",
    "                subprocess.run('rm ' + base_dir + '/num_files.txt', shell=True)\n",
    "                successful = True\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                attempts += 1\n",
    "        \n",
    "\n",
    "        # After files downloaded, move them to appropriate folder\n",
    "        subprocess.run(['rm', '-r', folder_path])\n",
    "        subprocess.run(['mkdir', folder_path])\n",
    "        shell_cmd = 'mv ' + base_dir + '/*.pdf ' + folder_path \n",
    "        subprocess.run(shell_cmd, shell=True)\n",
    "\n",
    "        # Record the filenames\n",
    "        shell_cmd = 'find ' + folder_path + '/*.pdf -maxdepth 1 -type f '\n",
    "        shell_cmd += '-printf \"%f\\n\" > ' + folder_path + '/file_names.txt'\n",
    "        subprocess.run(shell_cmd, shell=True)\n",
    "        with open(folder_path + '/file_names.txt') as f:\n",
    "            lines = f.readlines()\n",
    "        file_names.append(', '.join(lines).replace('\\n',''))\n",
    "        subprocess.run('rm ' + folder_path + '/file_names.txt', shell=True)\n",
    "\n",
    "        shell_cmd = 'pdfunite ' + folder_path + '/*.pdf ' + folder_path \n",
    "        shell_cmd += '/' + folder_name + '_combined.pdf'\n",
    "        subprocess.run(shell_cmd, shell=True)\n",
    "\n",
    "        driver.switch_to.default_content()\n",
    "        xpath = '//section[@class=\"modal fade modal-form '\n",
    "        xpath += 'modal-form-details in\"]/div/div/div/button'\n",
    "        close_button = driver.find_elements_by_xpath(xpath)\n",
    "        close_button[0].click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Append the downloaded row to the stored table and save\n",
    "        row = table.iloc[i]\n",
    "        stored_table = stored_table.append(row, ignore_index=True)\n",
    "        stored_table = stored_table.sort_values(\n",
    "            by='Date of notice', axis = 0, \n",
    "            ascending=False\n",
    "        )\n",
    "        stored_table = stored_table.reset_index(drop=True)\n",
    "        stored_table['Date of notice'] = stored_table['Date of notice'].apply(lambda x: x.strftime('%d/%m/%Y'))\n",
    "        stored_table.to_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv', index=False, header=True)\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-06T01:12:07.808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-7-b5c85f237f97>(96)scrape_page()\n",
      "-> successful = False\n",
      "(Pdb) l\n",
      " 91  \t            folder_name += '_' + str(folder_count+1)\n",
      " 92  \t        folder_path = base_dir + '/' + folder_name\n",
      " 93  \t\n",
      " 94  \t        import pdb; pdb.set_trace()\n",
      " 95  \t\n",
      " 96  ->\t        successful = False\n",
      " 97  \t        attempts = 0\n",
      " 98  \t        while not successful:\n",
      " 99  \t            if attempts > 5:\n",
      "100  \t                raise RuntimeError('Download timed out too many times.')\n",
      "101  \t            try:\n",
      "(Pdb) page_number\n",
      "23\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(97)scrape_page()\n",
      "-> attempts = 0\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(98)scrape_page()\n",
      "-> while not successful:\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(99)scrape_page()\n",
      "-> if attempts > 5:\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(101)scrape_page()\n",
      "-> try:\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(103)scrape_page()\n",
      "-> file_links[j].click()\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(104)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(102)scrape_page()\n",
      "-> for j in range(len(file_links)):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(107)scrape_page()\n",
      "-> file_count = 0\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(108)scrape_page()\n",
      "-> iterations = 0\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(109)scrape_page()\n",
      "-> while file_count < len(file_links):\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(110)scrape_page()\n",
      "-> if iterations > 600:\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(112)scrape_page()\n",
      "-> time.sleep(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(113)scrape_page()\n",
      "-> shell_cmd = '''find ''' + base_dir + '''/*.PDF -maxdepth 1 -exec sh -c 'mv \"$1\" \"${1%.PDF}.pdf\"' _ {} \\;'''\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(114)scrape_page()\n",
      "-> subprocess.run(shell_cmd, shell=True)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(115)scrape_page()\n",
      "-> shell_cmd = 'find ' + base_dir + '/*.pdf '\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(116)scrape_page()\n",
      "-> shell_cmd += '-type f -print | wc -l > '\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(117)scrape_page()\n",
      "-> shell_cmd += base_dir + '/num_files.txt'\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(118)scrape_page()\n",
      "-> subprocess.run(shell_cmd, shell=True)\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(119)scrape_page()\n",
      "-> file_count = int(np.loadtxt(base_dir + '/num_files.txt'))\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(120)scrape_page()\n",
      "-> iterations += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-7-b5c85f237f97>(109)scrape_page()\n",
      "-> while file_count < len(file_links):\n",
      "(Pdb) len(file_links)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,167):\n",
    "    \n",
    "    loading = True\n",
    "    attempts = 0\n",
    "    while loading:\n",
    "        if attempts > 30:\n",
    "            raise RuntimeError('Could not load website')\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source)\n",
    "            table = pd.read_html(soup.prettify())[0]            \n",
    "            if len(table) == 30:\n",
    "                loading = False\n",
    "        except:\n",
    "            attempts += 1\n",
    "\n",
    "    table = clean_columns(table)\n",
    "    table['Date of notice'] = pd.to_datetime(table['Date of notice'], dayfirst=True)\n",
    "    table.drop(labels='Actions', axis=1, inplace=True)  \n",
    "    \n",
    "    try:\n",
    "        stored_table = pd.read_csv('/home/student.unimelb.edu.au/shorte1/Documents/ACF_consulting/EPBC_notices.csv')\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)\n",
    "        shared = pd.merge(table, stored_table, how='left', indicator='Exist')\n",
    "        shared['Exist'] = np.where(shared.Exist == 'both', True, False)\n",
    "        exist = shared['Exist']\n",
    "        del shared\n",
    "    except:\n",
    "        stored_table = table.iloc[0:0]\n",
    "        stored_table['Date of notice'] = pd.to_datetime(stored_table['Date of notice'], dayfirst=True)\n",
    "        exist = [False]*30\n",
    "        exist = pd.Series(exist,name='Exist')\n",
    "     \n",
    "    if np.any(~exist):\n",
    "        scrape_page(driver, i, table, stored_table, exist)\n",
    "    \n",
    "    next_button = driver.find_elements_by_xpath(\n",
    "        '//a[@href=\"#\" and @data-page=\"' + str(i+1) + '\"]'\n",
    "    )[1]\n",
    "    ActionChains(driver).move_to_element(next_button).perform()\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    del table, stored_table\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
